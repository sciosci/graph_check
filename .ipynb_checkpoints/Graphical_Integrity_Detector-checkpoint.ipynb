{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from joblib import Parallel, delayed\n",
    "import os, shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shlex, subprocess, sys\n",
    "from PIL import Image\n",
    "from os import path\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.applications import ResNet152V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = 'Open_Access'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compound Figure Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = './open_access/'\n",
    "test_dir = './data/pipeline_data/'+test_name+'/'\n",
    "compound_dir = './data/pipeline_data/' + test_name+'/compound/'\n",
    "no_compound_dir = './data/pipeline_data/' + test_name+'/no_compound/'\n",
    "probability_dir = './data/pipeline_data/' + test_name+'/prob/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(test_dir)\n",
    "os.mkdir(compound_dir)\n",
    "os.mkdir(no_compound_dir)\n",
    "os.mkdir(probability_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_detector = keras.models.load_model('./data/weights/model_reproduce_final1')\n",
    "resnet_model = ResNet152V2(weights=\"imagenet\", pooling='max', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgs_to_compound_chunk_directly(target_dir,file_list,comp_model,chunk):\n",
    "    print('here')\n",
    "    if len(file_list) > chunk:\n",
    "        print('too many images')\n",
    "        return\n",
    "    x = []\n",
    "    predictions = []\n",
    "    procesed_file = []\n",
    "    print(len(file_list))\n",
    "    for file_name in file_list:\n",
    "        try:\n",
    "            img = load_img(target_dir+file_name,target_size=(224, 224))\n",
    "            img = image.img_to_array(img)\n",
    "            img = preprocess_input(np.expand_dims(img, axis=0))\n",
    "            x.append(img)\n",
    "            predictions.append(compound_detector.predict(img)[0])\n",
    "            procesed_file.append(file_name)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    images = np.vstack(x)\n",
    "    predictions = compound_detector.predict(images)\n",
    "    return procesed_file, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgs_to_compound(img_dir,compound_dir,comp_model,test_dir,output,chunk):\n",
    "    file_names = listdir(target_dir)\n",
    "    file_list = []\n",
    "    file_all = []\n",
    "    counter = 0\n",
    "    predictions = []\n",
    "    first = 0\n",
    "    rest = len(file_names)\n",
    "    if len(file_names)>chunk:\n",
    "        for file_name in file_names:\n",
    "            file_list.append(file_name)\n",
    "            counter += 1\n",
    "            rest = rest -1\n",
    "            if counter == chunk or rest==0:\n",
    "                print(counter)\n",
    "                file_list, predictions_1 = imgs_to_compound_chunk_directly(target_dir,file_list,comp_model,chunk)\n",
    "                file_all.append(file_list)\n",
    "                print('len_tmp: ', len(file_all))\n",
    "                counter = 0\n",
    "                file_list = []\n",
    "                if first == 0:\n",
    "                    predictions = predictions_1\n",
    "                    first = 1\n",
    "                else:\n",
    "                    predictions = np.concatenate((predictions, predictions_1))\n",
    "    else:\n",
    "        print('less than chunk')\n",
    "        for file_name in file_names:\n",
    "            file_list.append(file_name)\n",
    "        file_list, predictions = imgs_to_compound_chunk_directly(target_dir,file_list,comp_model,chunk)\n",
    "        file_all.append(file_list)\n",
    "        print('len_tmp2: ', len(file_all))\n",
    "        \n",
    "    f = open(test_dir+output, \"w\")\n",
    "    print('file_len: ',len(file_all))\n",
    "    print('prob_len: ',len(predictions))\n",
    "    predicted_label = []\n",
    "    probability_list = []\n",
    "    for j,pred in enumerate(predictions):\n",
    "        probability_list.append(pred[0])\n",
    "        if pred[0] > 0.50:\n",
    "            predicted_label.append('COMP')\n",
    "        else:\n",
    "            predicted_label.append('NOCOMP')\n",
    "\n",
    "    f.close()\n",
    "    def Extract(lst):\n",
    "        result = []\n",
    "        for i in range(len(lst)):\n",
    "            for j in range(len(lst[i])):\n",
    "                result.append(lst[i][j])\n",
    "        return result\n",
    "    \n",
    "    file_all = Extract(file_all)\n",
    "    print('file_len: ',len(file_all))\n",
    "    print('label_len: ',len(predicted_label))\n",
    "    print('prob_len: ', len(probability_list))\n",
    "    my_dict = {'Img': [file[:-4] for file in file_all], 'Type_Prediction': predicted_label, 'Prob': probability_list}\n",
    "    prediction = pd.DataFrame(my_dict)\n",
    "    prediction.to_csv(probability_dir + 'ImageClef_Comp.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_to_compound(target_dir,compound_dir,compound_detector,probability_dir,'ImageClef_Comp.txt',5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file to compound directory and not compound directory\n",
    "compound_df = pd.read_csv(probability_dir + 'ImageClef_Comp.csv')\n",
    "for i in range(len(compound_df)):\n",
    "    if compound_df['Type_Prediction'][i] == 'COMP':\n",
    "        name = compound_df['Img'][i] + '.jpg'\n",
    "        shutil.copy(target_dir+name, compound_dir+name)\n",
    "    else:\n",
    "        name = compound_df['Img'][i] + '.jpg'\n",
    "        shutil.copy(target_dir+name, no_compound_dir+name)\n",
    "# Count how many compound and nocompound images\n",
    "compound_df['Type_Prediction'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
