{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from joblib import Parallel, delayed\n",
    "import os, shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shlex, subprocess, sys\n",
    "from PIL import Image\n",
    "from os import path\n",
    "import math\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.applications import ResNet152V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Lib.MLG_lib import parse_lines_subplot,parse_line_subplot,parse_lines_text,parse_line_text_validate,parse_line_text,get_role,get_role_v "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = 'Open_Access'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compound Figure Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = './open_access/' # the folder which contains images\n",
    "test_dir = './data/pipeline_data/'+test_name+'/'\n",
    "compound_dir = './data/' + test_name+'/compound/'\n",
    "no_compound_dir = './data/' + test_name+'/no_compound/'\n",
    "probability_dir = './data/' + test_name+'/prob/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(test_dir)\n",
    "os.mkdir(compound_dir)\n",
    "os.mkdir(no_compound_dir)\n",
    "os.mkdir(probability_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_detector = keras.models.load_model('./data/weights/model_reproduce_final1')\n",
    "resnet_model = ResNet152V2(weights=\"imagenet\", pooling='max', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgs_to_compound_chunk_directly(target_dir,file_list,comp_model,chunk):\n",
    "    print('here')\n",
    "    if len(file_list) > chunk:\n",
    "        print('too many images')\n",
    "        return\n",
    "    x = []\n",
    "    predictions = []\n",
    "    procesed_file = []\n",
    "    print(len(file_list))\n",
    "    for file_name in file_list:\n",
    "        try:\n",
    "            img = load_img(target_dir+file_name,target_size=(224, 224))\n",
    "            img = image.img_to_array(img)\n",
    "            img = preprocess_input(np.expand_dims(img, axis=0))\n",
    "            x.append(img)\n",
    "            predictions.append(compound_detector.predict(img)[0])\n",
    "            procesed_file.append(file_name)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    images = np.vstack(x)\n",
    "    predictions = compound_detector.predict(images)\n",
    "    return procesed_file, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgs_to_compound(img_dir,compound_dir,comp_model,test_dir,output,chunk):\n",
    "    file_names = listdir(target_dir)\n",
    "    file_list = []\n",
    "    file_all = []\n",
    "    counter = 0\n",
    "    predictions = []\n",
    "    first = 0\n",
    "    rest = len(file_names)\n",
    "    if len(file_names)>chunk:\n",
    "        for file_name in file_names:\n",
    "            file_list.append(file_name)\n",
    "            counter += 1\n",
    "            rest = rest -1\n",
    "            if counter == chunk or rest==0:\n",
    "                print(counter)\n",
    "                file_list, predictions_1 = imgs_to_compound_chunk_directly(target_dir,file_list,comp_model,chunk)\n",
    "                file_all.append(file_list)\n",
    "                print('len_tmp: ', len(file_all))\n",
    "                counter = 0\n",
    "                file_list = []\n",
    "                if first == 0:\n",
    "                    predictions = predictions_1\n",
    "                    first = 1\n",
    "                else:\n",
    "                    predictions = np.concatenate((predictions, predictions_1))\n",
    "    else:\n",
    "        print('less than chunk')\n",
    "        for file_name in file_names:\n",
    "            file_list.append(file_name)\n",
    "        file_list, predictions = imgs_to_compound_chunk_directly(target_dir,file_list,comp_model,chunk)\n",
    "        file_all.append(file_list)\n",
    "        print('len_tmp2: ', len(file_all))\n",
    "        \n",
    "    f = open(test_dir+output, \"w\")\n",
    "    print('file_len: ',len(file_all))\n",
    "    print('prob_len: ',len(predictions))\n",
    "    predicted_label = []\n",
    "    probability_list = []\n",
    "    for j,pred in enumerate(predictions):\n",
    "        probability_list.append(pred[0])\n",
    "        if pred[0] > 0.50:\n",
    "            predicted_label.append('COMP')\n",
    "        else:\n",
    "            predicted_label.append('NOCOMP')\n",
    "\n",
    "    f.close()\n",
    "    def Extract(lst):\n",
    "        result = []\n",
    "        for i in range(len(lst)):\n",
    "            for j in range(len(lst[i])):\n",
    "                result.append(lst[i][j])\n",
    "        return result\n",
    "    \n",
    "    file_all = Extract(file_all)\n",
    "    print('file_len: ',len(file_all))\n",
    "    print('label_len: ',len(predicted_label))\n",
    "    print('prob_len: ', len(probability_list))\n",
    "    my_dict = {'Img': [file[:-4] for file in file_all], 'Type_Prediction': predicted_label, 'Prob': probability_list}\n",
    "    prediction = pd.DataFrame(my_dict)\n",
    "    prediction.to_csv(probability_dir + 'ImageClef_Comp.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_to_compound(target_dir,compound_dir,compound_detector,probability_dir,'ImageClef_Comp.txt',5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file to compound directory and not compound directory\n",
    "compound_df = pd.read_csv(probability_dir + 'ImageClef_Comp.csv')\n",
    "for i in range(len(compound_df)):\n",
    "    if compound_df['Type_Prediction'][i] == 'COMP':\n",
    "        name = compound_df['Img'][i] + '.jpg'\n",
    "        shutil.copy(target_dir+name, compound_dir+name)\n",
    "    else:\n",
    "        name = compound_df['Img'][i] + '.jpg'\n",
    "        shutil.copy(target_dir+name, no_compound_dir+name)\n",
    "# Return how many compound and nocompound images\n",
    "compound_df['Type_Prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(probability_dir+'ImageClef_Comp.csv', 'r') as inp, open(probability_dir+'Comp.txt', 'w') as out:\n",
    "    firstline = True\n",
    "    for line in inp:\n",
    "        if firstline:    #skip first line\n",
    "            firstline = False\n",
    "            continue\n",
    "        line = line.replace(',', ' ')\n",
    "        out.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subplot Detection with YoloV4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/home/thuang12/Desktop/Misleading_Graph'\n",
    "darknet_path = './darknet'\n",
    "working_path = './darknet/'\n",
    "voc_path = folder_path + '/data/data/voc_subplot_perf_orig_img.data'\n",
    "cfg_path = folder_path + '/data/cfg/yolov4_subplot_orig_img_perf_valid.cfg'\n",
    "weights_path = folder_path + '/data/yolov4_subplot_perf_orig_two_img_combined_and_all_original_4000.weights'\n",
    "compound_list_path = folder_path + '/data/pipeline_data/'+test_name+'/compound_list.txt'\n",
    "result_path = folder_path + '/data/pipeline_data/'+test_name+'/subplot_result1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = listdir(compound_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(compound_list_path, \"w\")\n",
    "for ff in file_names:\n",
    "    f.write(folder_path+compound_dir[1:]+ff+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "darknet_command = darknet_path+' detector test '+voc_path+' '+cfg_path+' '+weights_path+' '+ '-dont_show -ext_output < ' +compound_list_path+ ' > '+result_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = darknet_command\n",
    "print(args)\n",
    "p = subprocess.Popen(args,stdout=subprocess.PIPE,stderr=subprocess.PIPE,cwd=working_path, shell =True)\n",
    "p.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get yolo result\n",
    "f = open(result_path, \"r\")\n",
    "content = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = parse_lines_subplot(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(probability_dir +'Subplot.txt', \"w\")\n",
    "for line in lines:\n",
    "    file_name = line[0].split('/')[-1]\n",
    "    for i,prob in enumerate(line[2][1]):\n",
    "        f.write(str(file_name[:-4])+'_'+str(i) +'.png' + \",\" + str(prob) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Parallel(n_jobs=4)(delayed(parse_line_subplot)(lines[i],no_compound_dir) for i in range(len(lines)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_dir = './data/pipeline_data/'+test_name+'/Bar/'\n",
    "line_dir = './data/pipeline_data/'+test_name+'/Line/'\n",
    "heatmap_dir = './data/pipeline_data/'+test_name+'/Heatmap/'\n",
    "scatter_dir = './data/pipeline_data/'+test_name+'/Scatter/'\n",
    "violin_dir = './data/pipeline_data/'+test_name+'/Violin/'\n",
    "box_dir = './data/pipeline_data/'+test_name+'/Box/'\n",
    "area_dir = './data/pipeline_data/'+test_name+'/Area/'\n",
    "map_dir = './data/pipeline_data/'+test_name+'/Map/'\n",
    "table_dir = './data/pipeline_data/'+test_name+'/Table/'\n",
    "radar_dir = './data/pipeline_data/'+test_name+'/Radar/'\n",
    "pie_dir = './data/pipeline_data/'+test_name+'/Pie/'\n",
    "venn_dir = './data/pipeline_data/'+test_name+'/Venn/'\n",
    "pareto_dir = './data/pipeline_data/'+test_name+'/Pareto/'\n",
    "diagnostic_dir = './data/pipeline_data/'+test_name+'/Diagnostic/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(bar_dir)\n",
    "os.mkdir(line_dir)\n",
    "os.mkdir(heatmap_dir)\n",
    "os.mkdir(scatter_dir)\n",
    "os.mkdir(violin_dir)\n",
    "os.mkdir(box_dir)\n",
    "os.mkdir(area_dir)\n",
    "os.mkdir(map_dir)\n",
    "os.mkdir(table_dir)\n",
    "os.mkdir(radar_dir)\n",
    "os.mkdir(pie_dir)\n",
    "os.mkdir(venn_dir)\n",
    "os.mkdir(pareto_dir)\n",
    "os.mkdir(diagnostic_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_classifier = keras.models.load_model('./data/weights/image_classfier_covid_top_nn.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgs_to_type_chunck(target_dir,file_list,img_classifier,chunck):\n",
    "    print('here')\n",
    "    if len(file_list) > chunck:\n",
    "        print('too many images')\n",
    "        return\n",
    "    \n",
    "    x = []\n",
    "    my_file = []\n",
    "    \n",
    "    for file_name in file_list:\n",
    "        try:\n",
    "            img = load_img(target_dir+file_name,target_size=(224, 224))\n",
    "            img = image.img_to_array(img)\n",
    "            #print(file_name)\n",
    "            x.append(preprocess_input(np.expand_dims(img, axis=0)))\n",
    "            my_file.append(file_name)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    images = np.vstack(x)\n",
    "    features = resnet_model.predict(images)\n",
    "    features = features.reshape((features.shape[0], 2048))\n",
    "    \n",
    "    predictions = img_classifier.predict(features)\n",
    "    print(predictions[0])\n",
    "\n",
    "    return my_file, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgs_to_type(no_compound_dir,img_classifier,test_dir,output,chunck):\n",
    "    \n",
    "    #half_file_number = round(len(listdir(no_compound_dir))/2)\n",
    "    #file_names = listdir(no_compound_dir)[half_file_number:]\n",
    "    file_names = listdir(no_compound_dir)\n",
    "    \n",
    "    file_list = []\n",
    "    \n",
    "    counter = 0\n",
    "    predictions = []\n",
    "    first = 0\n",
    "    rest = len(file_names)\n",
    "    \n",
    "    predicted_file = []\n",
    "    \n",
    "    if len(file_names)>chunck:\n",
    "        for file_name in file_names:\n",
    "            file_list.append(file_name)\n",
    "            \n",
    "            counter += 1\n",
    "            rest = rest -1\n",
    "            if counter == chunck or rest==0:\n",
    "                print(counter)\n",
    "                files, predictions_1 = imgs_to_type_chunck(no_compound_dir,file_list,img_classifier,chunck)\n",
    "                counter = 0\n",
    "                file_list = []\n",
    "                if first == 0:\n",
    "                    predictions = predictions_1\n",
    "                    first = 1\n",
    "                else:\n",
    "                    predictions = np.concatenate((predictions, predictions_1))\n",
    "                \n",
    "                predicted_file += files\n",
    "                \n",
    "    else:\n",
    "        print('less than chunck')\n",
    "        for file_name in file_names:\n",
    "            try:\n",
    "                img = load_img(no_compound_dir+file_name,target_size=(224, 224))\n",
    "                file_list.append(file_name)\n",
    "            except:\n",
    "                file_names.remove(file_name)\n",
    "                continue\n",
    "        files, predictions = imgs_to_type_chunck(no_compound_dir,file_list,img_classifier,chunck)\n",
    "        predicted_file += files\n",
    "        \n",
    "    print(len(predicted_file))\n",
    "    f = open(probability_dir +'Bar.txt', \"w\")\n",
    "    \n",
    "    # \"Diagnostic\",\"BarGraph\", \"LineGraph\",'Map','ParetoChart','PieChart','RadarPlot','ScatterGraph','Table','VennDiagram','AreaGraph','ViolinPlot','Heatmap','BoxPlot']\n",
    "    for j,pred in enumerate(predictions):\n",
    "        if pred[1] > 0.90:\n",
    "            shutil.copy(no_compound_dir+predicted_file[j], bar_dir+predicted_file[j])\n",
    "            f.write(str(predicted_file[j]) + \",\" + str(pred[1]) + '\\n')\n",
    "        elif pred[10] > 0.90:\n",
    "            shutil.copy(no_compound_dir+predicted_file[j], area_dir+predicted_file[j])\n",
    "        elif pred[7] > 0.90:\n",
    "            shutil.copy(no_compound_dir+predicted_file[j], scatter_dir+predicted_file[j])\n",
    "        elif pred[12] > 0.90:\n",
    "            shutil.copy(no_compound_dir+predicted_file[j], heatmap_dir+predicted_file[j])\n",
    "        elif pred[2] > 0.90:\n",
    "            shutil.copy(no_compound_dir+predicted_file[j], line_dir+predicted_file[j])\n",
    "        elif pred[4] > 0.90:\n",
    "            shutil.copy(no_compound_dir+predicted_file[j], pareto_dir+predicted_file[j])\n",
    "        elif pred[11] > 0.90:\n",
    "            shutil.copy(no_compound_dir+predicted_file[j], violin_dir+predicted_file[j])\n",
    "        elif pred[0] > 0.90:\n",
    "            shutil.copy(no_compound_dir+predicted_file[j], diagnostic_dir+predicted_file[j])\n",
    "        elif pred[3] > 0.90:\n",
    "            shutil.copy(no_compound_dir+predicted_file[j], map_dir+predicted_file[j])\n",
    "        elif pred[5] > 0.9:\n",
    "            shutil.copy(no_compound_dir+predicted_file[j], pie_dir+predicted_file[j])\n",
    "        elif pred[6] > 0.9:\n",
    "            shutil.copy(no_compound_dir+predicted_file[j], radar_dir+predicted_file[j])\n",
    "        elif pred[8] > 0.9:\n",
    "            shutil.copy(no_compound_dir+predicted_file[j], table_dir+predicted_file[j])\n",
    "        elif pred[9] > 0.9:\n",
    "            shutil.copy(no_compound_dir+predicted_file[j], venn_dir+predicted_file[j])\n",
    "        elif pred[13] > 0.9:\n",
    "            shutil.copy(no_compound_dir+predicted_file[j], box_dir+predicted_file[j])\n",
    "        \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_to_type(no_compound_dir,img_classifier,bar_dir,'Bar_Prob.txt', 3500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('./' + test_name + '_swt/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stroke-Width-Transform "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Localization with YoloV4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/home/thuang12/Desktop/Misleading_Graph'\n",
    "target_dir = './OA_swt_AO/'\n",
    "working_path = './darknet'\n",
    "darknet_path = './darknet'\n",
    "voc_path = folder_path + '/data/data/voc_text_swt.data'\n",
    "cfg_path = folder_path + '/data/cfg/yolov4_text_swt_valid.cfg'\n",
    "weights_path = folder_path + '/data/yolov4_text_swt_last.weights'\n",
    "file_list_path = folder_path + '/data/pipeline_data/'+test_name+'/img_list.txt'\n",
    "result_path = folder_path + '/data/pipeline_data/'+test_name+'/text_result.txt'\n",
    "output_dir = folder_path + '/data/pipeline_data/'+test_name+'/text_localization/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = listdir(target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(file_list_path, \"w\")\n",
    "for ff in file_names:\n",
    "    if ff[-4:] in ['.png', '.jpg']:\n",
    "        f.write(folder_path+target_dir[1:]+ff+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "darknet_command = darknet_path+' detector test '+voc_path+' '+cfg_path+' '+weights_path+' '+ '-dont_show -ext_output < ' +file_list_path+ ' > '+result_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = darknet_command\n",
    "p = subprocess.Popen(args,stdout=subprocess.PIPE,stderr=subprocess.PIPE,cwd=working_path, shell = True)\n",
    "p.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(result_path, \"r\")\n",
    "content = f.readlines()\n",
    "f.close()\n",
    "len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = './OA_swt_AO/'\n",
    "data_path = './OA_swt_AO/'\n",
    "org_img_dir = './data/pipeline_data/'+test_name+'/Bar/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = parse_lines_text(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = listdir(org_img_dir)\n",
    "file_names = [file for file in file_names if file[-4:] in ['.png', '.jpg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the tmp folder\n",
    "tmp_list = listdir('./data/tmp')\n",
    "for i in range(len(tmp_list)):\n",
    "    shutil.rmtree('./data/tmp/' + tmp_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Parallel(n_jobs=4)(delayed(parse_line_text_validate)(lines[i],org_img_dir,labeled_data) for i in range(len(lines)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change all bar charts type from jpg to png\n",
    "all_img = listdir(labeled_data)\n",
    "for i in range(len(all_img)):\n",
    "    if all_img[i][-4:] == '.jpg':\n",
    "        im = Image.open(target_dir + all_img[i])\n",
    "        #im.save(target_dir + all_img[i][:-4] + '.png')\n",
    "        im.convert('RGB').save(target_dir + all_img[i][:-4] +'.png', \"PNG\", optimize=True)\n",
    "        os.remove(target_dir + all_img[i])\n",
    "        im.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run get role in rev environment \n",
    "- under rev folder\n",
    "- conda activate rev\n",
    "- python run_get_role.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(labeled_data_path, img_name, save_path):\n",
    "    for i in range(len(img_name)):\n",
    "        file_name = img_name[i][:-4]\n",
    "        #if path.exists(labeled_data_path+file_name+'-texts-all.csv'):\n",
    "        try:\n",
    "            df = pd.read_csv(labeled_data_path+file_name+'-texts-all.csv')\n",
    "            # get y label \n",
    "            y_label = df[df['type']=='y-axis-label'].value_counts('x')\n",
    "            y_label = y_label.iloc[:1]\n",
    "            if len(y_label) != 0:\n",
    "                y_label = y_label.index.tolist()[0][0]\n",
    "                width = float(np.mean(df[(df['type']=='y-axis-label') & (df['x'] == y_label)]['width']))\n",
    "                for j in range(len(df)):\n",
    "                    value = df['x'][j] + df['width'][j]\n",
    "                    value1 = df['x'][j]\n",
    "                    if df['type'][j] == 'y-axis-label' and (value < y_label + width - 6 or value > y_label + width + 6):\n",
    "                        df.at[j, 'type'] = 'y-axis-title'\n",
    "                    #if value > y_label+width-6 and value < y_label+width+6 and df['type'][j] != 'y-axis-label':\n",
    "                    if value1 >= y_label -3 and value1 <= y_label+width and df['type'][j] != 'y-axis-label':\n",
    "                        df.at[j, 'type'] = 'y-axis-label'\n",
    "                    #print('v:', value, 'y:', y_label+width)\n",
    "            # get x label\n",
    "            x_label = df[df['type']=='x-axis-label'].value_counts('y')\n",
    "            x_label = x_label.iloc[:1]\n",
    "\n",
    "            if len(x_label) != 0:\n",
    "                x_label = x_label.index.tolist()[0][0]\n",
    "                height = float(np.mean(df[(df['type']=='x-axis-label') & (df['y'] == x_label)]['height']))\n",
    "                for j in range(len(df)):\n",
    "                    value = df['y'][j]+height\n",
    "                    if df['type'][j] == 'x-axis-label' and (value < x_label+height-4 or value > x_label+height+4):\n",
    "                        df.at[j, 'type'] = 'x-axis-title'\n",
    "                    if value > x_label+height-4 and value < x_label+height+4 and df['type'][j] != 'x-axis-label':\n",
    "                        df.at[j, 'type'] = 'x-axis-label'\n",
    "\n",
    "            df.to_csv(save_path+file_name+'-texts-all.csv', index=False)\n",
    "            print(file_name, ' finished', )\n",
    "\n",
    "        except:\n",
    "            #df.to_csv(save_path+file_name+'-texts-all.csv', index=False)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = './OA_swt_AO/'\n",
    "file_names = listdir(labeled_data)\n",
    "bar_img = [ff for ff in file_names if ff[-4:] in ['.png', '.jpg']]\n",
    "save_path = './OA_swtAO_processed/'\n",
    "os.mkdir(save_path)\n",
    "pre_processing(labeled_data, bar_img, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save images to folder where processed csv files are\n",
    "no_process_path = './OA_swt_AO/'\n",
    "files = listdir(no_process_path)\n",
    "for i in range(len(files)):\n",
    "    if files[i][-4:] == '.png':\n",
    "        shutil.copy(no_process_path+files[i], './OA_swtAO_processed/'+files[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_dic(probability_dir):\n",
    "    #print(probability_dir)\n",
    "    #f1 = open(probability_dir +'Comp.txt', \"r\")\n",
    "    #lines_1 = f1.readlines()\n",
    "    #f1.close()\n",
    "    #f2 = open(probability_dir +'Subplot.txt', \"r\")\n",
    "    #lines_2 = f2.readlines()\n",
    "    #f2.close()\n",
    "    #f3 = open(probability_dir +'3D.txt', \"r\")\n",
    "    #lines_3 = f3.readlines()\n",
    "    #f3.close()\n",
    "    f4 = open(probability_dir +'Bar.txt', \"r\")\n",
    "    lines_4 = f4.readlines()\n",
    "    f4.close()\n",
    "    \"\"\"\n",
    "    comp_dic = {}\n",
    "    \n",
    "    for line in lines_1:\n",
    "        comp_dic[line.split(' ')[0]] = line.split(' ')[2][:-1]\n",
    "    \n",
    "    subp_dic = {}\n",
    "    \n",
    "    for line in lines_2:\n",
    "        subp_dic[line.split(',')[0]] = line.split(',')[1][:-1]\n",
    "    \n",
    "    d3_dic = {}\n",
    "    \n",
    "    #for line in lines_3:\n",
    "    #    d3_dic[line.split(',')[0]] = line.split(',')[1][:-1]\n",
    "    \"\"\"\n",
    "    \n",
    "    bar_dic = {}\n",
    "    \n",
    "    for line in lines_4:\n",
    "        bar_dic[line.split(',')[0].replace('.jpg', '.png')] = line.split(',')[1][:-1]\n",
    "        #bar_dic[line.split(',')[0]] = line.split(',')[1][:-1]\n",
    "    \n",
    "    #print(comp_dic)\n",
    "    \n",
    "    #return comp_dic,subp_dic,d3_dic,bar_dic\n",
    "    return bar_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_file(file,bar_dic):\n",
    "    #comp_prob = -1\n",
    "    #subp_prob = -1\n",
    "    #d3_prob = -1\n",
    "    bar_prob = -1\n",
    "    #if '_' in file:\n",
    "    #    comp_prob = comp_dic[file.split('_')[0]+'.png']\n",
    "    \"\"\"\n",
    "    if file[-7] == '_':\n",
    "        comp_prob = comp_dic[file[:-7]]\n",
    "        subp_prob = subp_dic[file]\n",
    "    elif file[-6] == '_':\n",
    "        comp_prob = comp_dic[file[:-6]]\n",
    "        subp_prob = subp_dic[file]\n",
    "    else:\n",
    "        comp_prob = comp_dic[file[:-4]]\n",
    "        subp_prob = -1\n",
    "    \"\"\"    \n",
    "    #d3_prob = d3_dic[file]\n",
    "    bar_prob = bar_dic[file]\n",
    "    \n",
    "    #print(d3_prob)\n",
    "    \n",
    "    #return comp_prob,subp_prob,d3_prob,bar_prob\n",
    "    return bar_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_digit(word):\n",
    "    word = str(word)\n",
    "    word = word.replace(',','')\n",
    "    word = word.replace('-','')\n",
    "    word = word.replace('|','')\n",
    "    try:\n",
    "        float(word)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_digit(word):\n",
    "    word = str(word)\n",
    "    word = word.replace(',','')\n",
    "    word = word.replace('-','')\n",
    "    word = word.replace('|','')\n",
    "    \n",
    "    return float(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_clear_files(file,labeled_data):\n",
    "    \n",
    "    try:\n",
    "        tb = pd.read_csv(labeled_data + file[:-4] + '-texts-all.csv')\n",
    "    except:\n",
    "        return 0\n",
    "        \n",
    "    y_axis_value = tb[tb['type'] == 'y-axis-label']['text'].tolist()\n",
    "    \n",
    "    if len(y_axis_value)<4:\n",
    "        return 0\n",
    "    #x_axis_pos_y = tb[tb['type'] == 'x-axis-label']['y'].tolist()\n",
    "    \n",
    "    #if len(x_axis_pos_y)<2:\n",
    "    #    return 0\n",
    "    \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually labeded data\n",
    "#misleading_label_dir = '/home/thuang12/Desktop/Misleading/'\n",
    "#other_label_dir = './data/pipeline_data/Open_Access/Bar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f = open('./OA_label/oa2_label.json',)\n",
    "data = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name  = []\n",
    "image_label = []\n",
    "for i in range(len(data)):\n",
    "    tmp_name = data[i]['image'].split('/')[-1][:-12]\n",
    "    image_name.append(tmp_name)\n",
    "    image_label.append(data[i].get('choice'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misleading_label_dir = []\n",
    "other_label_dir = []\n",
    "for i in range(len(image_name)):\n",
    "    if image_label[i] == 'misleading':\n",
    "        misleading_label_dir.append(image_name[i])\n",
    "    else:\n",
    "        other_label_dir.append(image_name[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extract_training(labeled_data, file,bar_dic):\n",
    "    y = -1\n",
    "    #if file in listdir(misleading_label_dir):\n",
    "    #    y = 1\n",
    "    #elif file in listdir(other_label_dir):\n",
    "    #    y = 0\n",
    "    #else:\n",
    "    #    return \n",
    "    \n",
    "    if file[:-4] in misleading_label_dir:\n",
    "        y = 1\n",
    "    elif file[:-4] in other_label_dir:\n",
    "        y = 0\n",
    "    else:\n",
    "        y = 0\n",
    "           \n",
    "            \n",
    "    im = cv2.imread(labeled_data+file)\n",
    "    \n",
    "    h,w,c = im.shape\n",
    "    \n",
    "    try:\n",
    "        tb = pd.read_csv(labeled_data + file[:-4] + '-texts-all.csv')\n",
    "    except:\n",
    "        return\n",
    "    #print(file)\n",
    "    #comp_prob,subp_prob,d3_prob,bar_prob = get_prob_file(file,comp_dic,subp_dic,d3_dic,bar_dic)\n",
    "    bar_prob = get_prob_file(file,bar_dic)\n",
    "    #print(d3_prob)\n",
    "\n",
    "    y_axis_value = tb[tb['type'] == 'y-axis-label']['text'].tolist()\n",
    "    y_axis_font = tb[tb['type'] == 'y-axis-label']['font'].tolist()\n",
    "    y_axis_prob = tb[tb['type'] == 'y-axis-label']['prob'].tolist()\n",
    "    y_axis_conf = tb[tb['type'] == 'y-axis-label']['conf'].tolist()\n",
    "\n",
    "    y_axis_pos_y = tb[tb['type'] == 'y-axis-label']['y'].tolist()\n",
    "    y_axis_pos_y = [float(yy) for index, yy in enumerate(y_axis_pos_y) if check_digit(y_axis_value[index])]\n",
    "\n",
    "    y_axis_value = [process_digit(yy) for yy in y_axis_value if check_digit(yy)]\n",
    "\n",
    "    y_axis_pairs = zip(y_axis_pos_y, y_axis_value)\n",
    "    res = sorted(y_axis_pairs, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    y_axis_pos_y = sorted(y_axis_pos_y)\n",
    "    y_axis_value = sorted(y_axis_value)\n",
    "    \n",
    "    steps = []\n",
    "    for index, yy in enumerate(y_axis_pos_y):\n",
    "        if index == len(y_axis_pos_y) - 1:\n",
    "            break\n",
    "        step = y_axis_pos_y[index + 1] - yy\n",
    "        steps.append(step)\n",
    "    \n",
    "    #if file == 'PMC3144924_pone.0022627.g003_2.png':\n",
    "    #    print(steps)\n",
    "    \n",
    "    if len(steps) >= 1:\n",
    "        average_step = np.mean(steps)\n",
    "    else:\n",
    "        average_step = -1\n",
    "        \n",
    "    #print(average_step)\n",
    "    increase_rates = []\n",
    "    decimal = 0\n",
    "    for index, yy in enumerate(y_axis_value):           \n",
    "        if index < len(y_axis_value) -1 and yy.is_integer() != y_axis_value[index+1].is_integer():\n",
    "            decimal += 1\n",
    "            \n",
    "    for index, yy in enumerate(y_axis_value):\n",
    "        if index == len(y_axis_value) - 1:\n",
    "            break\n",
    "        try:\n",
    "            increase_rate = (y_axis_value[index + 1] - yy) / steps[index]\n",
    "            increase_rates.append(increase_rate)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    increase_rate_std = np.std(increase_rates/np.mean(increase_rates))\n",
    "    \n",
    "    #common_rates,rates_count = common_rough(increase_rates)\n",
    "    #if len(common_rates) == 1:\n",
    "    #    increase_rate_std = 0\n",
    "    #else:\n",
    "    #    increase_rate_std = np.std(common_rates)\n",
    "    \n",
    "    \n",
    "    #if decimal >= 2:\n",
    "    #    average_increase_rate = 0\n",
    "    #else:\n",
    "    average_increase_rate = np.mean(increase_rates)\n",
    "\n",
    "    x_axis_pos_y = tb[tb['type'] == 'x-axis-label']['y'].tolist()\n",
    "    x_axis_pos_y = [float(xx) for xx in x_axis_pos_y]\n",
    "\n",
    "    average_x_axis_pos_y = np.mean(x_axis_pos_y)\n",
    "\n",
    "    if np.std(x_axis_pos_y) > 2:\n",
    "        # print(\"double x axes\")\n",
    "        x_axis_pos_y = [float(xx) for xx in x_axis_pos_y if xx > average_x_axis_pos_y]\n",
    "    average_x_axis_pos_y = np.mean(x_axis_pos_y)\n",
    "\n",
    "    inference_flag_1 = 0\n",
    "    min_y_1 = 0\n",
    "    # print(res)\n",
    "    # print(average_step)\n",
    "    if len(res) <= 3:\n",
    "        return []\n",
    "    \n",
    "    #if abs(res[0][0] - average_x_axis_pos_y) > abs(average_step / 2) and check_digit(res[1][1]) and res[-1][0] < average_x_axis_pos_y:\n",
    "        # inference\n",
    "        # print('inference one')\n",
    "    #    inference_flag_1 = 1\n",
    "        # print(res[0][1])\n",
    "        # print(average_increase_rate)\n",
    "        # print(abs(res[0][0] - average_x_axis_pos_y))\n",
    "    #    min_y_1 = res[0][1] - average_increase_rate * (res[0][0] - average_x_axis_pos_y)\n",
    "\n",
    "    ##### try to find the bar in the plot #####\n",
    "    #if res[0][1] != 0:\n",
    "    y_ratio = res[0][0] / h\n",
    "    if min(y_axis_value)!=0 and y_ratio<0.85:\n",
    "        min_y_1 = res[0][1] - average_increase_rate * average_step    \n",
    "        inference_flag_1 = 1\n",
    "        #if file == 'PMC3144924_pone.0022627.g003_2.png':\n",
    "        #    print('res: ', res[0][1], average_increase_rate, average_step)\n",
    "        #    print('min_y: ', min_y_1)\n",
    "    #inference_flag_2 = 0\n",
    "    #if not check_digit(res[0][1]) and check_digit(res[1][1]) and inference_flag_1 == 0:\n",
    "        # inference\n",
    "        # print('inference two')\n",
    "    #    inference_flag_2 = 1\n",
    "    #    min_y_1 = res[1][1] - average_increase_rate * (res[1][0] - x_axis_pos_y)\n",
    "\n",
    "    #increase_rate_std = np.std(increase_rates)\n",
    "    inference_flag = 0\n",
    "    if inference_flag_1 == 0: #and inference_flag_2 == 0:\n",
    "        #min_y = res[0][1]/(max(y_axis_value)+1)\n",
    "        min_y = min(y_axis_value)\n",
    "    else:\n",
    "        inference_flag = 1\n",
    "        #print('assign')\n",
    "        #print(inference_flag_1)\n",
    "        #print(min_y_1)\n",
    "        #min_y = min_y_1/(max(y_axis_value)+1)\n",
    "        #min_y = min(y_axis_value)\n",
    "        min_y = min_y_1\n",
    "        \n",
    "    if math.isnan(min_y):\n",
    "        #print(len(res))\n",
    "        min_y = 0\n",
    "\n",
    "    if np.isnan(increase_rate_std):\n",
    "        increase_rate_std = 0\n",
    "\n",
    "    if np.isnan(average_x_axis_pos_y):\n",
    "        average_x_axis_pos_y = -1\n",
    "    \n",
    "    #(res[0][0] - average_x_axis_pos_y)\n",
    "    #return [y, [inference_flag ,(res[0][0] - average_x_axis_pos_y),  min_y, increase_rate_std,np.mean(y_axis_font),np.mean(y_axis_prob),np.mean(y_axis_conf), float(comp_prob),float(subp_prob),float(bar_prob)]]    \n",
    "    #return [y, [[y ,file],inference_flag,min_y,decimal,np.mean(y_axis_prob),np.mean(y_axis_conf),float(bar_prob),increase_rate_std]]\n",
    "    return [y, [inference_flag,min_y,decimal,np.mean(y_axis_prob),np.mean(y_axis_conf),float(bar_prob),increase_rate_std]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_trainining(img_list,labeled_data,bar_dic):\n",
    "    X = []\n",
    "    y = []\n",
    "    files = []\n",
    "    for i in range(len(img_list)):\n",
    "        if keep_clear_files(img_list[i],labeled_data):\n",
    "            #res = feature_extract_training(labeled_data,img_list[i],comp_dic,subp_dic,d3_dic,bar_dic)\n",
    "            res = feature_extract_training(labeled_data,img_list[i],bar_dic)\n",
    "            if res != None and len(res) == 2 and len(res[1])==7:\n",
    "                X.append(res[1])\n",
    "                y.append(res[0])\n",
    "                files.append(img_list[i])\n",
    "\n",
    "    return X,y,files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labeled_data = './OA_swt2_processed/'\n",
    "#probability_dir = './data/pipeline_data/'+test_name+'/prob/'\n",
    "labeled_data = '/home/thuang12/Desktop/FN_goodtext/'\n",
    "probability_dir = './data/pipeline_data/Open_Access2/prob/'\n",
    "file_names = listdir(labeled_data)\n",
    "bar_img = [ff for ff in file_names if ff[-4:] in ['.png', '.jpg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comp_dic,subp_dic,d3_dic,bar_dic = get_prob_dic(probability_dir)\n",
    "bar_dic = get_prob_dic(probability_dir)\n",
    "#X,y,files = get_features_trainining(bar_img,labeled_data,comp_dic,subp_dic,d3_dic,bar_dic)\n",
    "X,y,files = get_features_trainining(bar_img,labeled_data,bar_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphical Integrity Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphical_integrity_classification = RandomForestClassifier(max_depth=7, random_state=1, n_estimators = 500, oob_score=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X,y, test_size=0.2, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphical_integrity_classification.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = graphical_integrity_classification.predict(test_X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
